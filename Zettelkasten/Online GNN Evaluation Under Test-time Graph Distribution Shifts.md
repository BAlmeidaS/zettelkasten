---
tags:
  - paper
  - rate-35
  - year-2024
  - reviewed
---
URL: https://arxiv.org/abs/2403.09953
Review: https://openreview.net/forum?id=KbetDM33YG
Review average (1-5): 3.5
Conf: ICLR 2024

---

This paper considers the problem of evaluating online GNNs, specifically tackling the issue of train-test distribution shift. The contributions of the paper revolve around solving this problem specifically by taking into account real-world constraints, such as inability to access the train graph at test time.

The reviews said that the paper narrow down too much, although also agreed that it was a justified decision.

---

The paper address the issue of accessing a gnn model using the data after its training, trying to identify possible shifts. Most of the old researches (related work) deal with euclidean space (rather easier than graph structural data) <blue highlighted on the paper>.

"As our proposed LEBED serves as an online GNN evaluation metric, we report all the results based on the correlation between our LEBED scores and the ground-truth test error, using two correlation metrics: the linear correlation R2 (ranging from 0 to 1) and Spearman’s rank correlation (ranging from -1 to 1). Negative values in Spearman’s rank correlation indicate a negative correlation with ground-truth test errors."

The downside of this approach is that it depends of another GNN training equally complex to the original training. <appendix c>

Semi-supervised or self-supervised learning topic.

"During the re-training stage, the meaning of "supervision" for online GNN model evaluation is a little different from model training. Here, the pseudo labels $Y_{te}^{∗}$ are not specifically for supervising GNN to achieve the optimal performance on the test graph, but as a reference guidance to quantify the divergence between the pseudo-labels $Y_{te}^{∗}$ generated by the pre-trained GNN models vs. the predicted labels $\hat{Y}_{te}$ by the re-trained GNN models.

In this case, it can be used for "the supervision of re-training", allowing for evaluating how well the re-trained models can adapt to and predict for the test graph, compared to the initial pre-trained models."