---
tags:
  - paper
  - rate-35
  - year-2022
---
URL: https://arxiv.org/abs/2205.12454
Review: https://openreview.net/forum?id=lMMaNf6oxKM
Review average (1-5): 3.5
Conf: NeurIPS 2022

---

This paper presents a powerful, general, scalable, and linearly complex graph Transformer. Positional encodings and structural encodings are redefined with local, global, and relative categories, and an attempt has been made to include  local and global focus attentions in a graph Transformer.

In this work, the authors propose a recipe on how to build a general, powerful, scalable graph Transformer with linear complexity and state-of-the-art results on a diverse set of benchmarks.

The authors present a way how to efficiently use transformers on graph data. They report modular architecture, containing of graph positional encodings, structural encodings, and graph features, that further passed to an ensemble of arbitrary transformer block and arbitrary message passing neural network.