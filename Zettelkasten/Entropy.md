---
tags:
  - math
  - Statistics
---
*Entropy* $\displaystyle \large H(P)$ measures the *amount of uncertainty or randomness in a probability distribution $\displaystyle \large P$*. 

It quantifies how unpredictable or random the outcomes of a distribution are

$$\displaystyle \Huge \begin{eqnarray} 
H(P) = -\sum_xP(x)*\log P(x)
\end{eqnarray}$$

Usually the base of the $\displaystyle \large \log$ is 2, making the units of entropy *bits*.

Since, entropy measures the "amount of randomness" the *highest entropy is on a [[Uniform Distribution]]*, because all outcomes are equally likely (no bias, completely random).

When comparing two distributions, the one with *higher [[Variance]] will have higher [[Entropy]]


