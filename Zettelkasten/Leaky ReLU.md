---
tags:
  - deep-learning
---
Similar to [[ReLU function|ReLU]] but on the negative $\displaystyle \large x$ the [[derivative]] is not zero:

$$\displaystyle \Huge \begin{eqnarray} 
f(x) = 
\begin{cases}
x,& \text{if } x\gt0\\
ax,&
\text{otherwise}
\end{cases}
\end{eqnarray}$$

![[Pasted image 20240212195447.png|400]]

The [[derivative]] so is:
$$\displaystyle \Huge \begin{eqnarray} 
f^\prime(x) = 
\begin{cases}
1,& \text{if } x\gt0\\
a,&
\text{otherwise}
\end{cases}
\end{eqnarray}$$