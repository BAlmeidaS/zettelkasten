---
tags:
  - paper
  - rate-no
---
URL: https://arxiv.org/abs/2012.09699
Review: -
Review average (1-5):
Conf: CoRR 2020

---

The paper titled "A Generalization of Transformer Networks to Graphs" by Vijay Prakash Dwivedi and Xavier Bresson introduces a novel graph transformer architecture that extends the capabilities of the traditional transformer model, originally designed for natural language processing (NLP), to handle arbitrary graph structures. The proposed model incorporates several key features: an attention mechanism based on neighborhood connectivity, positional encoding using Laplacian eigenvectors, and batch normalization for improved training efficiency and generalization. Additionally, the architecture supports edge feature representations, enhancing its applicability to domains like chemistry and knowledge graphs. Numerical experiments on benchmark datasets demonstrate that this graph transformer outperforms existing graph neural network (GNN) models, providing a robust baseline for future research at the intersection of attention mechanisms and graph data.

Does not have open review, but it has 610 citations