---
tags:
  - paper
---
URL: https://arxiv.org/pdf/2306.02376
Review: -
Conf: ICML 2023


---

They explain well why deep attention on [[Graph Attention Networks|Graph Attention Networks]] are hard to be made