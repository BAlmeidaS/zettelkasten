---
tags:
  - paper
  - rate-35
---
URL: https://arxiv.org/abs/2306.10759
Review: https://openreview.net/forum?id=R4xpvDTWkV
Review average (1-5): 35
Conf: NeurIPS 2023

---

SGFormer adapts the transformer to large graphs by replacing the usual quadratic self-attention mechanism with a convex combination of linear attention and an arbitrary Graph Neural Network (GNN). The benchmark using node prediction tasks from Open Graph Benchmark (OGB) and achieve SOTA results. Moreover, the method is fast and scales to large graphs. They provide some theoretical justification why their one layer method can be as good as multiple layers.

On the Reviews, the first review is an extensive and valuable discussion