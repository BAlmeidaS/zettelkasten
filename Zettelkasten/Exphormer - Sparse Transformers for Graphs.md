---
tags:
  - paper
  - rate-35
---
URL: https://arxiv.org/pdf/2303.06147
Review: -
Review average (1-5):
Conf: ICML 2024

---

Good presentation on: https://icml.cc/virtual/2023/poster/23782

The paper introduces EXPHORMER, a framework for scalable graph transformers. EXPHORMER combines sparse attention mechanisms based on virtual global nodes and expander graphs. These mechanisms provide desirable mathematical properties, such as spectral expansion and sparsity, allowing the model to scale linearly with the graph size while maintaining competitive accuracy.

The framework integrates into the [[Recipe for a General, Powerful, Scalable Graph Transformer]] architecture and achieves state-of-the-art results on various graph datasets. This approach enables handling larger graphs more efficiently than previous transformer-based models.

Also it is connected to [[Graph Transformer Networks]]

